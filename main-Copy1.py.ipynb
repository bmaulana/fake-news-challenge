{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from data.scorer import score_submission, print_confusion_matrix, score_defaults, SCORE_REPORT\n",
    "from nltk import word_tokenize\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import cosine  # TODO may have to implement own maths formulas\n",
    "from scipy.stats import entropy  # (kl-divergence) TODO may have to implement own maths formulas\n",
    "from sklearn.linear_model import LogisticRegression  # TODO implement own classifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # TODO implement own tokenizer + count + tf-idf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder  # TODO implement my own mapping\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "f_bodies = open('data/train_bodies.csv', 'r', encoding='utf-8')\n",
    "csv_bodies = csv.DictReader(f_bodies)\n",
    "bodies = []\n",
    "for row in csv_bodies:\n",
    "    body_id = int(row['Body ID'])\n",
    "    if (body_id + 1) > len(bodies):\n",
    "        bodies += [None] * (body_id + 1 - len(bodies))\n",
    "    bodies[body_id] = row['articleBody']\n",
    "f_bodies.close()\n",
    "body_inverse_index = {bodies[i]: i for i in range(len(bodies))}\n",
    "\n",
    "all_unrelated, all_discuss, all_agree, all_disagree = [], [], [], []  # each article = (headline, body, stance)\n",
    "\n",
    "f_stances = open('data/train_stances.csv', 'r', encoding='utf-8')\n",
    "csv_stances = csv.DictReader(f_stances)\n",
    "for row in csv_stances:\n",
    "    body = bodies[int(row['Body ID'])]\n",
    "    if row['Stance'] == 'unrelated':\n",
    "        all_unrelated.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'discuss':\n",
    "        all_discuss.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'agree':\n",
    "        all_agree.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'disagree':\n",
    "        all_disagree.append((row['Headline'], body, row['Stance']))\n",
    "f_stances.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tUnrltd\tDiscuss\t Agree\tDisagree\n",
      "All\t 36545 \t 8909 \t 3678 \t 840\n",
      "Train\t 32890 \t 8018 \t 3310 \t 756\n",
      "Valid.\t 3655 \t 891 \t 368 \t 84\n"
     ]
    }
   ],
   "source": [
    "# Split to train and validation 9:1\n",
    "print('\\tUnrltd\\tDiscuss\\t Agree\\tDisagree')\n",
    "print('All\\t', len(all_unrelated), '\\t', len(all_discuss), '\\t', len(all_agree), '\\t', len(all_disagree))\n",
    "\n",
    "train_unrelated = all_unrelated[:len(all_unrelated) * 9 // 10]\n",
    "train_discuss = all_discuss[:len(all_discuss) * 9 // 10]\n",
    "train_agree = all_agree[:len(all_agree) * 9 // 10]\n",
    "train_disagree = all_disagree[:len(all_disagree) * 9 // 10]\n",
    "\n",
    "val_unrelated = all_unrelated[len(all_unrelated) * 9 // 10:]\n",
    "val_discuss = all_discuss[len(all_discuss) * 9 // 10:]\n",
    "val_agree = all_agree[len(all_agree) * 9 // 10:]\n",
    "val_disagree = all_disagree[len(all_disagree) * 9 // 10:]\n",
    "\n",
    "print('Train\\t', len(train_unrelated), '\\t', len(train_discuss), '\\t', len(train_agree), '\\t', len(train_disagree))\n",
    "print('Valid.\\t', len(val_unrelated), '\\t', len(val_discuss), '\\t', len(val_agree), '\\t', len(val_disagree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (Total) (44974, 3) \tValidation (Total) (4998, 3)\n",
      "32890 \t 8018 \t 3310 \t 756\n",
      "3655 \t 891 \t 368 \t 84\n"
     ]
    }
   ],
   "source": [
    "train_all = train_unrelated + train_discuss + train_agree + train_disagree  # each article = (headline, body, stance)\n",
    "random.Random(0).shuffle(train_all)\n",
    "train_all = np.array(train_all)\n",
    "\n",
    "val_all = val_unrelated + val_discuss + val_agree + val_disagree\n",
    "random.Random(0).shuffle(val_all)\n",
    "val_all = np.array(val_all)\n",
    "\n",
    "print('Train (Total)', train_all.shape, '\\tValidation (Total)', val_all.shape)\n",
    "print(np.count_nonzero(train_all[:, 2] == 'unrelated'), '\\t',\n",
    "      np.count_nonzero(train_all[:, 2] == 'discuss'), '\\t',\n",
    "      np.count_nonzero(train_all[:, 2] == 'agree'), '\\t',\n",
    "      np.count_nonzero(train_all[:, 2] == 'disagree'))\n",
    "print(np.count_nonzero(val_all[:, 2] == 'unrelated'), '\\t',\n",
    "      np.count_nonzero(val_all[:, 2] == 'discuss'), '\\t',\n",
    "      np.count_nonzero(val_all[:, 2] == 'agree'), '\\t',\n",
    "      np.count_nonzero(val_all[:, 2] == 'disagree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al Qaida hostage killed in raid ['al', 'qaida', 'hostage', 'killed', 'in', 'raid']\n",
      "UPDATE: Internet report of Ebola outbreak in Purdon not true ['update', 'internet', 'report', 'of', 'ebola', 'outbreak', 'in', 'purdon', 'not', 'true']\n",
      "Christian Bale Won't Play Steve Jobs After All ['christian', 'bale', 'wont', 'play', 'steve', 'jobs', 'after', 'all']\n",
      "#RIPFidel fuels false rumors of Cuban leader's death after another Fidel Castro dies ['ripfidel', 'fuels', 'false', 'rumors', 'of', 'cuban', 'leaders', 'death', 'after', 'another', 'fidel', 'castro', 'dies']\n",
      "Boko Haram claims to have German hostage, denies ceasefire ['boko', 'haram', 'claims', 'to', 'have', 'german', 'hostage', 'denies', 'ceasefire']\n",
      "Batmobile Stolen From 'Batman V Superman: Dawn Of Justice' Set? Report Claims That One Of The Batmobile Models Is Missing! Detroit Locals Responsible? ['batmobile', 'stolen', 'from', 'batman', 'v', 'superman', 'dawn', 'of', 'justice', 'set', 'report', 'claims', 'that', 'one', 'of', 'the', 'batmobile', 'models', 'is', 'missing', 'detroit', 'locals', 'responsible']\n",
      "Report: ISIS kidnaps Canadian-Israeli, former IDF soldier who went to fight with the Kurds ['report', 'isis', 'kidnaps', 'canadian', 'israeli', 'former', 'idf', 'soldier', 'who', 'went', 'to', 'fight', 'with', 'the', 'kurds']\n",
      "Accused Boston Marathon Bomber Severely Injured In Prison, May Never Walk Or Talk Again ['accused', 'boston', 'marathon', 'bomber', 'severely', 'injured', 'in', 'prison', 'may', 'never', 'walk', 'or', 'talk', 'again']\n",
      "ISIS Appears To Behead American Photojournalist In YouTube Video ['isis', 'appears', 'to', 'behead', 'american', 'photojournalist', 'in', 'youtube', 'video']\n",
      "An attorney representing a man who says he realized he had inadvertently recorded audio of the Michael Brown shooting has provided that audio to CNN:\n",
      "\n",
      "Attorney Lopa Blumenthal says the recording was made during a video chat. The audio captures what seem like 10 gunshot sounds—an initial group of six, then a pause, then four more. An autopsy of Brown's body performed at his family's request by an experienced forensic pathologist named Michael Baden indicated Brown was hit by at least six shots.\n",
      "\n",
      "Eyewitness Dorian Johnson, a friend of Brown's, has said that officer Darren Wilson fired several shots at Brown after Brown had turned toward him and raised his hands. The Baden autopsy indicated that Brown had been shot exclusively in the front of his body.\n",
      "\n",
      "Blumenthal says the man who made the recording has spoken to the FBI.\n",
      "\n",
      "Read more on Ferguson. ['an', 'attorney', 'representing', 'a', 'man', 'who', 'says', 'he', 'realized', 'he', 'had', 'inadvertently', 'recorded', 'audio', 'of', 'the', 'michael', 'brown', 'shooting', 'has', 'provided', 'that', 'audio', 'to', 'cnn', 'attorney', 'lopa', 'blumenthal', 'says', 'the', 'recording', 'was', 'made', 'during', 'a', 'video', 'chat', 'the', 'audio', 'captures', 'what', 'seem', 'like', '10', 'gunshot', 'soundsan', 'initial', 'group', 'of', 'six', 'then', 'a', 'pause', 'then', 'four', 'more', 'an', 'autopsy', 'of', 'browns', 'body', 'performed', 'at', 'his', 'familys', 'request', 'by', 'an', 'experienced', 'forensic', 'pathologist', 'named', 'michael', 'baden', 'indicated', 'brown', 'was', 'hit', 'by', 'at', 'least', 'six', 'shots', 'eyewitness', 'dorian', 'johnson', 'a', 'friend', 'of', 'browns', 'has', 'said', 'that', 'officer', 'darren', 'wilson', 'fired', 'several', 'shots', 'at', 'brown', 'after', 'brown', 'had', 'turned', 'toward', 'him', 'and', 'raised', 'his', 'hands', 'the', 'baden', 'autopsy', 'indicated', 'that', 'brown', 'had', 'been', 'shot', 'exclusively', 'in', 'the', 'front', 'of', 'his', 'body', 'blumenthal', 'says', 'the', 'man', 'who', 'made', 'the', 'recording', 'has', 'spoken', 'to', 'the', 'fbi', 'read', 'more', 'on', 'ferguson']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(\"[^a-zA-Z0-9 ]+\")\n",
    "def tokenise(text):\n",
    "    text = pattern.sub('', text.replace('\\n', ' ').replace('-', ' ').lower())\n",
    "    return word_tokenize(text)\n",
    "\n",
    "for i in range(9):\n",
    "    print(train_all[i, 0], tokenise(train_all[i, 0]))\n",
    "print(train_all[0, 1], tokenise(train_all[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al Qaida hostage killed in raid defaultdict(<class 'float'>, {'al': 1.0, 'qaida': 1.0, 'hostage': 1.0, 'killed': 1.0, 'in': 1.0, 'raid': 1.0})\n",
      "UPDATE: Internet report of Ebola outbreak in Purdon not true defaultdict(<class 'float'>, {'update': 1.0, 'internet': 1.0, 'report': 1.0, 'of': 1.0, 'ebola': 1.0, 'outbreak': 1.0, 'in': 1.0, 'purdon': 1.0, 'not': 1.0, 'true': 1.0})\n",
      "Christian Bale Won't Play Steve Jobs After All defaultdict(<class 'float'>, {'christian': 1.0, 'bale': 1.0, 'wont': 1.0, 'play': 1.0, 'steve': 1.0, 'jobs': 1.0, 'after': 1.0, 'all': 1.0})\n",
      "#RIPFidel fuels false rumors of Cuban leader's death after another Fidel Castro dies defaultdict(<class 'float'>, {'ripfidel': 1.0, 'fuels': 1.0, 'false': 1.0, 'rumors': 1.0, 'of': 1.0, 'cuban': 1.0, 'leaders': 1.0, 'death': 1.0, 'after': 1.0, 'another': 1.0, 'fidel': 1.0, 'castro': 1.0, 'dies': 1.0})\n",
      "Boko Haram claims to have German hostage, denies ceasefire defaultdict(<class 'float'>, {'boko': 1.0, 'haram': 1.0, 'claims': 1.0, 'to': 1.0, 'have': 1.0, 'german': 1.0, 'hostage': 1.0, 'denies': 1.0, 'ceasefire': 1.0})\n",
      "Batmobile Stolen From 'Batman V Superman: Dawn Of Justice' Set? Report Claims That One Of The Batmobile Models Is Missing! Detroit Locals Responsible? defaultdict(<class 'float'>, {'batmobile': 2.0, 'stolen': 1.0, 'from': 1.0, 'batman': 1.0, 'v': 1.0, 'superman': 1.0, 'dawn': 1.0, 'of': 2.0, 'justice': 1.0, 'set': 1.0, 'report': 1.0, 'claims': 1.0, 'that': 1.0, 'one': 1.0, 'the': 1.0, 'models': 1.0, 'is': 1.0, 'missing': 1.0, 'detroit': 1.0, 'locals': 1.0, 'responsible': 1.0})\n",
      "Report: ISIS kidnaps Canadian-Israeli, former IDF soldier who went to fight with the Kurds defaultdict(<class 'float'>, {'report': 1.0, 'isis': 1.0, 'kidnaps': 1.0, 'canadian': 1.0, 'israeli': 1.0, 'former': 1.0, 'idf': 1.0, 'soldier': 1.0, 'who': 1.0, 'went': 1.0, 'to': 1.0, 'fight': 1.0, 'with': 1.0, 'the': 1.0, 'kurds': 1.0})\n",
      "Accused Boston Marathon Bomber Severely Injured In Prison, May Never Walk Or Talk Again defaultdict(<class 'float'>, {'accused': 1.0, 'boston': 1.0, 'marathon': 1.0, 'bomber': 1.0, 'severely': 1.0, 'injured': 1.0, 'in': 1.0, 'prison': 1.0, 'may': 1.0, 'never': 1.0, 'walk': 1.0, 'or': 1.0, 'talk': 1.0, 'again': 1.0})\n",
      "ISIS Appears To Behead American Photojournalist In YouTube Video defaultdict(<class 'float'>, {'isis': 1.0, 'appears': 1.0, 'to': 1.0, 'behead': 1.0, 'american': 1.0, 'photojournalist': 1.0, 'in': 1.0, 'youtube': 1.0, 'video': 1.0})\n",
      "An attorney representing a man who says he realized he had inadvertently recorded audio of the Michael Brown shooting has provided that audio to CNN:\n",
      "\n",
      "Attorney Lopa Blumenthal says the recording was made during a video chat. The audio captures what seem like 10 gunshot sounds—an initial group of six, then a pause, then four more. An autopsy of Brown's body performed at his family's request by an experienced forensic pathologist named Michael Baden indicated Brown was hit by at least six shots.\n",
      "\n",
      "Eyewitness Dorian Johnson, a friend of Brown's, has said that officer Darren Wilson fired several shots at Brown after Brown had turned toward him and raised his hands. The Baden autopsy indicated that Brown had been shot exclusively in the front of his body.\n",
      "\n",
      "Blumenthal says the man who made the recording has spoken to the FBI.\n",
      "\n",
      "Read more on Ferguson. defaultdict(<class 'float'>, {'an': 3.0, 'attorney': 2.0, 'representing': 1.0, 'a': 4.0, 'man': 2.0, 'who': 2.0, 'says': 3.0, 'he': 2.0, 'realized': 1.0, 'had': 3.0, 'inadvertently': 1.0, 'recorded': 1.0, 'audio': 3.0, 'of': 5.0, 'the': 8.0, 'michael': 2.0, 'brown': 5.0, 'shooting': 1.0, 'has': 3.0, 'provided': 1.0, 'that': 3.0, 'to': 2.0, 'cnn': 1.0, 'lopa': 1.0, 'blumenthal': 2.0, 'recording': 2.0, 'was': 2.0, 'made': 2.0, 'during': 1.0, 'video': 1.0, 'chat': 1.0, 'captures': 1.0, 'what': 1.0, 'seem': 1.0, 'like': 1.0, '10': 1.0, 'gunshot': 1.0, 'soundsan': 1.0, 'initial': 1.0, 'group': 1.0, 'six': 2.0, 'then': 2.0, 'pause': 1.0, 'four': 1.0, 'more': 2.0, 'autopsy': 2.0, 'browns': 2.0, 'body': 2.0, 'performed': 1.0, 'at': 3.0, 'his': 3.0, 'familys': 1.0, 'request': 1.0, 'by': 2.0, 'experienced': 1.0, 'forensic': 1.0, 'pathologist': 1.0, 'named': 1.0, 'baden': 2.0, 'indicated': 2.0, 'hit': 1.0, 'least': 1.0, 'shots': 2.0, 'eyewitness': 1.0, 'dorian': 1.0, 'johnson': 1.0, 'friend': 1.0, 'said': 1.0, 'officer': 1.0, 'darren': 1.0, 'wilson': 1.0, 'fired': 1.0, 'several': 1.0, 'after': 1.0, 'turned': 1.0, 'toward': 1.0, 'him': 1.0, 'and': 1.0, 'raised': 1.0, 'hands': 1.0, 'been': 1.0, 'shot': 1.0, 'exclusively': 1.0, 'in': 1.0, 'front': 1.0, 'spoken': 1.0, 'fbi': 1.0, 'read': 1.0, 'on': 1.0, 'ferguson': 1.0})\n"
     ]
    }
   ],
   "source": [
    "def doc_to_tf(text):\n",
    "    words = tokenise(text)\n",
    "    ret = defaultdict(float)\n",
    "    for word in words:\n",
    "        ret[word] += 1.0\n",
    "    return ret\n",
    "    \n",
    "for i in range(9):\n",
    "    print(train_all[i, 0], doc_to_tf(train_all[i, 0]))\n",
    "print(train_all[0, 1], doc_to_tf(train_all[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An attorney representing a man who says he realized he had inadvertently recorded audio of the Michael Brown shooting has provided that audio to CNN:\n",
      "\n",
      "Attorney Lopa Blumenthal says the recording was made during a video chat. The audio captures what seem like 10 gunshot sounds—an initial group of six, then a pause, then four more. An autopsy of Brown's body performed at his family's request by an experienced forensic pathologist named Michael Baden indicated Brown was hit by at least six shots.\n",
      "\n",
      "Eyewitness Dorian Johnson, a friend of Brown's, has said that officer Darren Wilson fired several shots at Brown after Brown had turned toward him and raised his hands. The Baden autopsy indicated that Brown had been shot exclusively in the front of his body.\n",
      "\n",
      "Blumenthal says the man who made the recording has spoken to the FBI.\n",
      "\n",
      "Read more on Ferguson.\n",
      "Al Qaida hostage killed in raid\n"
     ]
    }
   ],
   "source": [
    "# Build corpus of article bodies and headlines in training dataset\n",
    "corpus = np.r_[train_all[:, 1], train_all[:, 0]]  # 0 to 44973 are bodies, 44974 to 89943 are headlines\n",
    "\n",
    "print(corpus[0])\n",
    "print(corpus[44974])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 89948/89948 [01:58<00:00, 758.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('an', 32525.0), ('attorney', 1738.0), ('representing', 417.0), ('a', 48652.0), ('man', 13433.0), ('who', 28184.0), ('says', 11259.0), ('he', 29818.0), ('realized', 518.0), ('had', 26590.0)]\n"
     ]
    }
   ],
   "source": [
    "# Learn idf of every word in the corpus\n",
    "df = defaultdict(float)\n",
    "for doc in tqdm(corpus):\n",
    "    words = tokenise(doc)\n",
    "    seen = set()\n",
    "    for word in words:\n",
    "        if word not in seen:\n",
    "            df[word] += 1.0\n",
    "            seen.add(word)\n",
    "\n",
    "print(list(df.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 24569/24569 [00:00<00:00, 386671.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('an', 2.017203073541107), ('attorney', 4.945932607675217), ('representing', 6.371516689504744), ('a', 1.6145293713628024), ('man', 2.90145403620269), ('who', 2.1604529215138246), ('says', 3.077986220335818), ('he', 2.1040970688136422), ('realized', 6.155094238863611), ('had', 2.2186700303762477)]\n"
     ]
    }
   ],
   "source": [
    "num_docs = corpus.shape[0]\n",
    "idf = defaultdict(float)\n",
    "for word, val in tqdm(df.items()):\n",
    "    idf[word] = np.log((1.0 + num_docs) / (1.0 + val)) + 1.0  # smoothed idf\n",
    "\n",
    "print(list(idf.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLoVe word vectors\n",
    "f_glove = open(\"data/glove.6B.50d.txt\", \"rb\")  # download from https://nlp.stanford.edu/projects/glove/\n",
    "# also try if higher-dimension/higher-vocabulary GLoVe vectors perform better\n",
    "glove_vectors = {}\n",
    "for line in tqdm(f_glove):\n",
    "    glove_vectors[str(line.split()[0]).split(\"'\")[1]] = np.array(list(map(float, line.split()[1:])))\n",
    "# for key, value in glove_vectors.items():\n",
    "#    print(key, value)\n",
    "#    break\n",
    "print(glove_vectors['glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a document to GloVe vectors, by computing tf-idf of each word * GLoVe of word / total tf-idf for document\n",
    "def doc_to_glove(doc):\n",
    "    doc_tf_idf = tf_idf_vec.transform([doc])\n",
    "    # print(doc_tf_idf[:10])\n",
    "    _, cols = doc_tf_idf[0].nonzero()\n",
    "    doc_vector = np.array([0.0]*50)\n",
    "    sum_tf_idf = 0\n",
    "    for col in cols:\n",
    "        word = col_to_word[col]\n",
    "        if word in glove_vectors:\n",
    "            # print(word, tf_idf[row, col], glove_vectors[word])\n",
    "            doc_vector += glove_vectors[word] * doc_tf_idf[0, col]\n",
    "            sum_tf_idf += doc_tf_idf[0, col]\n",
    "    doc_vector /= sum_tf_idf\n",
    "    return doc_vector\n",
    "print(train_all[0,0])\n",
    "print(doc_to_glove(train_all[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity of GLoVe vectors for all headline-body pairs\n",
    "def cosine_similarity(doc):\n",
    "    return 1.0 - cosine(doc_to_glove(doc[0]), doc_to_glove(doc[1]))\n",
    "\n",
    "for i in range(10):\n",
    "    # unrelated should have lower than rest\n",
    "    print(cosine_similarity(train_all[i]), train_all[i, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the KL-Divergence of language model (LM) representations of the headline and the body\n",
    "eps = 0.1  # add a small value for every common word in the LM, as KL-divergence won't work if there are 0 values\n",
    "def kl_divergence(doc):\n",
    "    # Convert body and headline to bag of words representations\n",
    "    vec = CountVectorizer(stop_words='english')\n",
    "    vec_all = vec.fit_transform([doc[0], doc[1]])\n",
    "    vec_headline = np.squeeze(np.array(vec_all[0].todense()))\n",
    "    vec_body = np.squeeze(np.array(vec_all[1].todense()))\n",
    "    \n",
    "    # Compute a simple unigram LM of headline and body using bag of words / no. of words in doc\n",
    "    lm_headline = (vec_headline + eps) / np.sum(vec_headline)\n",
    "    lm_body = (vec_body + eps) / np.sum(vec_body)\n",
    "    \n",
    "    # Return KL-divergence of lm\n",
    "    return entropy(lm_headline, lm_body)\n",
    "\n",
    "for i in range(10):\n",
    "    # unrelated should have lower than rest\n",
    "    print(cosine_similarity(train_all[i]), train_all[i, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other feature 1 - distinguish unrelated vs rest\n",
    "def ngram_match(doc):\n",
    "    # Returns how many times n-grams (up to 3-gram) that occur in the article's headline occur on the article's body.\n",
    "    vec = CountVectorizer(stop_words='english', ngram_range=(1, 3))\n",
    "    vec.fit([doc[0]])\n",
    "    vec_body = vec.transform([doc[1]])\n",
    "    return np.power((np.sum(vec_body) / len(doc[1])), 1 / np.e)\n",
    "    \n",
    "for i in range(10):\n",
    "    # unrelated should have lower than rest\n",
    "    print(ngram_match(train_all[i]), train_all[i, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other feature 2 - distinguish discuss vs agree/disagree\n",
    "def body_polarity(doc):\n",
    "    # TODO (should be lower for discuss compared to agree/disagree)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other feature 3 - distinguish agree vs disagree\n",
    "sent_analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_mismatch(doc):\n",
    "    return np.absolute(sent_analyser.polarity_scores(doc[0])['compound'] - sent_analyser.polarity_scores(doc[1])['compound'])\n",
    "# sentiment_mismatch doesn't do anything atm.. still predicts everything as discuss\n",
    "# Values seem to be random (agree not neccesarily lower, disagree not neccesarily higher)\n",
    "# Maybe use a feature based on no. of negative words similar to 'no's in the body? - more likely to be disagree\n",
    "    \n",
    "train_related = train_all[train_all[:, 2] == 'discuss']\n",
    "for i in range(5):\n",
    "    print(sentiment_mismatch(train_related[i]), train_related[i, 2])\n",
    "train_agree = train_all[train_all[:, 2] == 'agree']\n",
    "for i in range(5):\n",
    "    # Should be lower\n",
    "    print(sentiment_mismatch(train_agree[i]), train_agree[i, 2])\n",
    "train_disagree = train_all[train_all[:, 2] == 'disagree']\n",
    "for i in range(5):\n",
    "    # Should be higher\n",
    "    print(sentiment_mismatch(train_disagree[i]), train_disagree[i, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to convert (headline, body) to feature vectors for each document\n",
    "ftrs = [cosine_similarity, kl_divergence, ngram_match]\n",
    "def to_feature_array(doc):\n",
    "    vec = np.array([0.0] * len(ftrs))\n",
    "    for i in range(len(ftrs)):\n",
    "        vec[i] = ftrs[i](doc)\n",
    "    return vec\n",
    "\n",
    "# Initialise x and y for train dataset\n",
    "x_train = np.array([to_feature_array(doc) for doc in tqdm(train_all)])\n",
    "print(x_train[:5])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(list(train_all[:, 2]))\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GLoVe distance vs KL-Divergence on a coloured scatter plot with different colours for each label\n",
    "colours = np.array(['g', 'r', 'b', 'y'])\n",
    "plt.scatter(list(x_train[:, 0]), list(x_train[:, 1]), c=colours[y_train])\n",
    "plt.xlabel('Cosine Similarity of GLoVe vectors')\n",
    "plt.ylabel('KL Divergence of Unigram LMs')\n",
    "print([(colours[i], le.classes_[i]) for i in range(len(le.classes_))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise x and y for validation dataset\n",
    "x_val = np.array([to_feature_array(doc) for doc in tqdm(val_all)])\n",
    "print(x_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement linear/logistic regression classifier using these features. Optimise params on validation set\n",
    "clf = LogisticRegression(C=1e2, random_state=0, multi_class='multinomial', solver='saga')\n",
    "# clf = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)  # TODO temporary classifier\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict y for validation set\n",
    "y_pred = clf.predict(x_val)\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset format for score_submission in scorer.py\n",
    "predicted = le.inverse_transform(y_pred)\n",
    "body_ids = [str(body_inverse_index[body]) for body in val_all[:, 1]]\n",
    "pred_for_cm = np.array([{'Headline': val_all[i, 0], 'Body ID': body_ids[i], 'Stance': predicted[i]} for i in range(len(val_all))])\n",
    "gold_for_cm = np.array([{'Headline': val_all[i, 0], 'Body ID': body_ids[i], 'Stance': val_all[i, 2]} for i in range(len(val_all))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score using scorer.py:\n",
    "test_score, cm = score_submission(gold_for_cm, pred_for_cm)\n",
    "null_score, max_score = score_defaults(gold_for_cm)\n",
    "print_confusion_matrix(cm)\n",
    "print(SCORE_REPORT.format(max_score, null_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to .csv in the correct format (see data/scorer.py and data/train_stances.csv)\n",
    "pred_tosave = np.array([('Headline','Body ID','Stance')] + [(val_all[i, 0], body_ids[i], predicted[i]) for i in range(len(val_all))])\n",
    "gold_tosave = np.array([('Headline','Body ID','Stance')] + [(val_all[i, 0], body_ids[i], val_all[i, 2]) for i in range(len(val_all))])\n",
    "\n",
    "# TODO does not work ATM (UnicodeDecodeError) - try 'python scorer.py val_gold.csv val_predicted.csv' in data folder\n",
    "np.savetxt('data/val_predicted.csv', pred_tosave, delimiter=',', fmt='%s', encoding='utf-8', newline='\\n')\n",
    "np.savetxt('data/val_gold.csv', gold_tosave, delimiter=',', fmt='%s', encoding='utf-8', newline='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO analyse importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoostingClassifier(n_estimators=200)\n",
    "ACCURACY: 0.834  ||  1604.25  ||  No Added Features (just Cosine and KL-Divergence)\n",
    "ACCURACY: 0.861  ||  1743.5   ||  'Raw' Unigram Match\n",
    "ACCURACY: 0.861  ||  1743.5   ||  1+log(x) Unigram Match\n",
    "ACCURACY: 0.860  ||  1743.5   ||  1-1/e^(x/2) Unigram Match\n",
    "ACCURACY: 0.884  ||  1834.5   ||  x/len(body) Unigram Match\n",
    "ACCURACY: 0.873  ||  1790.5   ||  log(x)/log(len(body)) Unigram Match\n",
    "ACCURACY: 0.884  ||  1834.25  ||  (x/len(body))^1/e Unigram Match\n",
    "    \n",
    "LogisticRegression(C=1e5)\n",
    "ACCURACY: 0.880  ||  1795.5   ||  (x/len(body))^1/e Unigram Match\n",
    "\n",
    "LogisticRegression(C=1e2, multi_class='multinomial', solver='saga')\n",
    "ACCURACY: 0.881  ||  1818.5   ||  (x/len(body))^1/e Unigram Match\n",
    "ACCURACY: 0.882  ||  1819.5   ||  (x/len(body))^1/e 2-gram Match\n",
    "ACCURACY: 0.882  ||  1820.5   ||  (x/len(body))^1/e 3-gram Match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
