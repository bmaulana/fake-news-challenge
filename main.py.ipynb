{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import entropy\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "f_bodies = open('data/train_bodies.csv', 'r', encoding='utf-8')\n",
    "csv_bodies = csv.DictReader(f_bodies)\n",
    "bodies = []\n",
    "for row in csv_bodies:\n",
    "    body_id = int(row['Body ID'])\n",
    "    if (body_id + 1) > len(bodies):\n",
    "        bodies += [None] * (body_id + 1 - len(bodies))\n",
    "    bodies[body_id] = row['articleBody']\n",
    "f_bodies.close()\n",
    "\n",
    "all_unrelated, all_discuss, all_agree, all_disagree = [], [], [], []  # each article = (headline, body, stance)\n",
    "\n",
    "f_stances = open('data/train_stances.csv', 'r', encoding='utf-8')\n",
    "csv_stances = csv.DictReader(f_stances)\n",
    "for row in csv_stances:\n",
    "    body = bodies[int(row['Body ID'])]\n",
    "    if row['Stance'] == 'unrelated':\n",
    "        all_unrelated.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'discuss':\n",
    "        all_discuss.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'agree':\n",
    "        all_agree.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'disagree':\n",
    "        all_disagree.append((row['Headline'], body, row['Stance']))\n",
    "f_stances.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train and validation\n",
    "print('\\tUnrltd\\tDiscuss\\t Agree\\tDisagree')\n",
    "print('All\\t', len(all_unrelated), '\\t', len(all_discuss), '\\t', len(all_agree), '\\t', len(all_disagree))\n",
    "\n",
    "train_unrelated = all_unrelated[:len(all_unrelated) * 9 // 10]\n",
    "train_discuss = all_discuss[:len(all_discuss) * 9 // 10]\n",
    "train_agree = all_agree[:len(all_agree) * 9 // 10]\n",
    "train_disagree = all_disagree[:len(all_disagree) * 9 // 10]\n",
    "\n",
    "val_unrelated = all_unrelated[len(all_unrelated) * 9 // 10:]\n",
    "val_discuss = all_discuss[len(all_discuss) * 9 // 10:]\n",
    "val_agree = all_agree[len(all_agree) * 9 // 10:]\n",
    "val_disagree = all_disagree[len(all_disagree) * 9 // 10:]\n",
    "\n",
    "print('Train\\t', len(train_unrelated), '\\t', len(train_discuss), '\\t', len(train_agree), '\\t', len(train_disagree))\n",
    "print('Valid.\\t', len(val_unrelated), '\\t', len(val_discuss), '\\t', len(val_agree), '\\t', len(val_disagree))\n",
    "\n",
    "train_all = np.array(train_unrelated + train_discuss + train_agree + train_disagree)  # each article = (headline, body, stance)\n",
    "random.Random(0).shuffle(train_all)\n",
    "val_all = np.array(val_unrelated + val_discuss + val_agree + val_disagree)\n",
    "random.Random(0).shuffle(val_all)\n",
    "print('Train (Total)', train_all.shape, '\\tValidation (Total)', val_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLoVe word vectors\n",
    "f_glove = open(\"data/glove.6B.50d.txt\", \"rb\")  # download from https://nlp.stanford.edu/projects/glove/\n",
    "# also try if higher-dimension/higher-vocabulary GLoVe vectors work better\n",
    "glove_vectors = {}\n",
    "for line in tqdm(f_glove):\n",
    "    glove_vectors[str(line.split()[0]).split(\"'\")[1]] = np.array(list(map(float, line.split()[1:])))\n",
    "# for key, value in glove_vectors.items():\n",
    "#    print(key, value)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert documents to tf-idf form\n",
    "corpus = np.r_[train_all[:, 1], train_all[:, 0]]  # 0 to 44973 are bodies, 44974 to 89943 are headlines\n",
    "vectoriser = CountVectorizer(stop_words='english')\n",
    "bag_of_words = vectoriser.fit_transform(corpus)\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tf_idf = transformer.fit_transform(bag_of_words)\n",
    "# print(tf_idf[0])\n",
    "print(tf_idf.shape)  # (2 x no of docs, no of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert documents to GloVe vectors, by computing tf-idf of each word * GLoVe of word / total tf-idf for document\n",
    "doc_vectors = []\n",
    "col_to_word = vectoriser.get_feature_names()\n",
    "# print(train_all[0][1])\n",
    "for row in tqdm(range(tf_idf.shape[0])):  # testing: start from train_all.shape[0]\n",
    "    _, cols = tf_idf[row].nonzero()\n",
    "    doc_vector = np.array([0.0]*50)\n",
    "    sum_tf_idf = 0\n",
    "    for col in cols:\n",
    "        word = col_to_word[col]\n",
    "        if word in glove_vectors:\n",
    "            # print(word, tf_idf[row, col], glove_vectors[word])\n",
    "            doc_vector += glove_vectors[word] * tf_idf[row, col]\n",
    "            sum_tf_idf += tf_idf[row, col]\n",
    "    doc_vector /= sum_tf_idf\n",
    "    # print(doc_vector)\n",
    "    doc_vectors.append(doc_vector)\n",
    "    # break\n",
    "doc_vectors = np.array(doc_vectors)\n",
    "print(doc_vectors.shape)\n",
    "print(doc_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity of GLoVe vectors for all headline-body pairs\n",
    "features = []  # indices correspond to train_all\n",
    "for i in tqdm(range(train_all.shape[0])):\n",
    "    features.append({'headline_body_similarity': 1.0 - cosine(doc_vectors[i], doc_vectors[i+train_all.shape[0]])})\n",
    "\n",
    "for i in range(20):\n",
    "    print(features[i]['headline_body_similarity'], train_all[i][2])  # unrelated should have lower than rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the KL-Divergence of language model (LM) representations of the headline and the body\n",
    "\n",
    "eps = 0.1  # add a small value for every common word in the LM, as KL-divergence won't work if there are 0-values\n",
    "for i in tqdm(range(train_all.shape[0])):\n",
    "    # Find 1-grams (columns) that exist in either body or headline\n",
    "    _, cols_body = bag_of_words[i].nonzero()\n",
    "    _, cols_headline = bag_of_words[i + train_all.shape[0]].nonzero()\n",
    "    cols_merged = np.union1d(cols_body, cols_headline)\n",
    "    # Remove all other columns\n",
    "    vec_body = np.squeeze(np.array(bag_of_words[i, cols_merged].todense()))\n",
    "    vec_headline = np.squeeze(np.array(bag_of_words[i + train_all.shape[0], cols_merged].todense()))\n",
    "    \n",
    "    # Compute a simple unigram LM using bag of words / no. of words in doc\n",
    "    lm_body = (vec_body + eps) / np.sum(vec_body)\n",
    "    lm_headline = (vec_headline + eps) / np.sum(vec_headline)\n",
    "    \n",
    "    features[i]['kl_divergence'] = entropy(lm_body, lm_headline)\n",
    "\n",
    "for i in range(20):\n",
    "    print(features[i]['kl_divergence'], train_all[i][2])  # unrelated should have higher than rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO other features (use fnc-baseline's n-grams + entities (maybe polarity of sentences containing entities in body also))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise x and y for train dataset\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(list(train_all[:, 2]))\n",
    "print(y_train)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "x_train = vectorizer.fit_transform(features).todense()\n",
    "print(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Plot GLoVe distance vs KL-Divergence on a coloured scatter plot with different colours for each label\n",
    "colours = np.array(['b', 'g', 'r', 'y'])\n",
    "plt.scatter(list(x_train[:, 0]), list(x_train[:, 1]), c=colours[y_train])\n",
    "plt.xlabel('Cosine Similarity of GLoVe vectors')\n",
    "plt.ylabel('KL Divergence of Unigram LMs')\n",
    "print([(colours[i], le.classes_[i]) for i in range(len(le.classes_))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement linear/logistic regression classifier using these features. Optimise params on validation set\n",
    "clf = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO test performance of classifier on test set (confusion matrix, precision, recall, F-score) - use scorer\n",
    "# TODO have to make all of preprocessing a function... from converting to tf-idf form onwards\n",
    "# and for tf-idf, need a way to calculate tf-idf of each new document without recalculating idf... \n",
    "# use existing idf weights from the training set: stackoverflow.com/questions/45232671/obtain-tf-idf-weights-of-words-with-sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO analyse importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused code, could be useful again later\n",
    "\n",
    "# Generate language model representations of headlines and bodies (bag of words / no. of words in doc for a 1-gram LM)\n",
    "# Sparse matrix division, because regular '/' would convert it to dense which will run out of memory\n",
    "# Equivalent to lang_models = bag_of_words/bag_of_words.sum(axis=1)[:, 0] \n",
    "# (Credit to https://stackoverflow.com/questions/42225269/scipy-sparse-matrix-division)\n",
    "b = sparse.bsr_matrix(bag_of_words)\n",
    "c = sparse.diags(1/b.sum(axis=1).A.ravel())\n",
    "lang_models = c @ b\n",
    "\n",
    "print(lang_models.sum(axis=1)) # every element should be 1\n",
    "print(lang_models.shape, bag_of_words.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
