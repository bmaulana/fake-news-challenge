{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "f_bodies = open('data/train_bodies.csv', 'r', encoding='utf-8')\n",
    "csv_bodies = csv.DictReader(f_bodies)\n",
    "bodies = []\n",
    "for row in csv_bodies:\n",
    "    body_id = int(row['Body ID'])\n",
    "    if (body_id + 1) > len(bodies):\n",
    "        bodies += [None] * (body_id + 1 - len(bodies))\n",
    "    bodies[body_id] = row['articleBody']\n",
    "f_bodies.close()\n",
    "\n",
    "all_unrelated, all_discuss, all_agree, all_disagree = [], [], [], []  # each article = (headline, body, stance)\n",
    "\n",
    "f_stances = open('data/train_stances.csv', 'r', encoding='utf-8')\n",
    "csv_stances = csv.DictReader(f_stances)\n",
    "for row in csv_stances:\n",
    "    body = bodies[int(row['Body ID'])]\n",
    "    if row['Stance'] == 'unrelated':\n",
    "        all_unrelated.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'discuss':\n",
    "        all_discuss.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'agree':\n",
    "        all_agree.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'disagree':\n",
    "        all_disagree.append((row['Headline'], body, row['Stance']))\n",
    "f_stances.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train and validation\n",
    "print('\\tUnrltd\\tDiscuss\\t Agree\\tDisagree')\n",
    "print('All\\t', len(all_unrelated), '\\t', len(all_discuss), '\\t', len(all_agree), '\\t', len(all_disagree))\n",
    "\n",
    "train_unrelated = all_unrelated[:len(all_unrelated) * 9 // 10]\n",
    "train_discuss = all_discuss[:len(all_discuss) * 9 // 10]\n",
    "train_agree = all_agree[:len(all_agree) * 9 // 10]\n",
    "train_disagree = all_disagree[:len(all_disagree) * 9 // 10]\n",
    "\n",
    "val_unrelated = all_unrelated[len(all_unrelated) * 9 // 10:]\n",
    "val_discuss = all_discuss[len(all_discuss) * 9 // 10:]\n",
    "val_agree = all_agree[len(all_agree) * 9 // 10:]\n",
    "val_disagree = all_disagree[len(all_disagree) * 9 // 10:]\n",
    "\n",
    "print('Train\\t', len(train_unrelated), '\\t', len(train_discuss), '\\t', len(train_agree), '\\t', len(train_disagree))\n",
    "print('Valid.\\t', len(val_unrelated), '\\t', len(val_discuss), '\\t', len(val_agree), '\\t', len(val_disagree))\n",
    "\n",
    "train_all = train_unrelated + train_discuss + train_agree + train_disagree  # each article = (headline, body, stance)\n",
    "random.Random(0).shuffle(train_all)\n",
    "val_all = val_unrelated + val_discuss + val_agree + val_disagree\n",
    "random.Random(0).shuffle(val_all)\n",
    "print('Train (Total)', len(train_all), '\\tValidation (Total)', len(val_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert documents to tf-idf form\n",
    "corpus = [i[1] for i in train_all] + [i[0] for i in train_all]  # 0 to 44973 are bodies, 44974 to 89943 are headlines\n",
    "vectoriser = CountVectorizer(stop_words='english')\n",
    "matrix = vectoriser.fit_transform(corpus)\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tf_idf = transformer.fit_transform(matrix)\n",
    "# print(tf_idf[0])\n",
    "print(tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLoVe word vectors\n",
    "f_glove = open(\"data/glove.6B.50d.txt\", \"rb\")  # download from https://nlp.stanford.edu/projects/glove/\n",
    "glove_vectors = {}\n",
    "for line in f_glove:\n",
    "    glove_vectors[str(line.split()[0]).split(\"'\")[1]] = np.array(list(map(float, line.split()[1:])))\n",
    "# for key, value in glove_vectors.items():\n",
    "#    print(key, value)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert documents to GloVe vectors, by computing tf-idf of each word * GLoVe of word / total tf-idf for document\n",
    "doc_vectors = []\n",
    "col_to_word = vectoriser.get_feature_names()\n",
    "# print(train_all[0][1])\n",
    "for row in range(tf_idf.shape[0]):  # testing: from len(train_all)\n",
    "    _, cols = tf_idf[row].nonzero()\n",
    "    doc_vector = np.array([0.0]*50)\n",
    "    sum_tf_idf = 0\n",
    "    for col in cols:\n",
    "        word = col_to_word[col]\n",
    "        if word in glove_vectors:\n",
    "            # print(word, tf_idf[row, col], glove_vectors[word])\n",
    "            doc_vector += glove_vectors[word] * tf_idf[row, col]\n",
    "            sum_tf_idf += tf_idf[row, col]\n",
    "    doc_vector /= sum_tf_idf\n",
    "    # print(doc_vector)\n",
    "    doc_vectors.append(doc_vector)\n",
    "    # break\n",
    "doc_vectors = np.array(doc_vectors)\n",
    "print(doc_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity of GLoVe vectors for all headline-body pairs\n",
    "features = []  # indices correspond to train_all\n",
    "for i in range(len(train_all)):\n",
    "    features.append({'headline_body_similarity': 1.0 - cosine(doc_vectors[i], doc_vectors[i+len(train_all)])})\n",
    "\n",
    "for i in range(20):\n",
    "    print(features[i]['headline_body_similarity'], train_all[i][2])  # unrelated should have lower than rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO language model of headline and body (use bag of words / no. of words in doc for a 1-gram LM?)\n",
    "# TODO KL-Divergence between each headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO other features (use fnc-baseline's n-grams + entities (maybe polarity of sentences containing entities in body also))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Plot GLoVe distance vs KL-Divergence on a coloured scatter plot with different colours for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement linear/logistic regression classifier using these features. Optimise params on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO test performance of classifier on test set (confusion matrix, precision, recall, F-score) - use scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO analyse importance of each feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
