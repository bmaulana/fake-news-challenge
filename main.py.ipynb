{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from data.scorer import score_submission, print_confusion_matrix\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import entropy\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "f_bodies = open('data/train_bodies.csv', 'r', encoding='utf-8')\n",
    "csv_bodies = csv.DictReader(f_bodies)\n",
    "bodies = []\n",
    "for row in csv_bodies:\n",
    "    body_id = int(row['Body ID'])\n",
    "    if (body_id + 1) > len(bodies):\n",
    "        bodies += [None] * (body_id + 1 - len(bodies))\n",
    "    bodies[body_id] = row['articleBody']\n",
    "f_bodies.close()\n",
    "\n",
    "all_unrelated, all_discuss, all_agree, all_disagree = [], [], [], []  # each article = (headline, body, stance)\n",
    "\n",
    "f_stances = open('data/train_stances.csv', 'r', encoding='utf-8')\n",
    "csv_stances = csv.DictReader(f_stances)\n",
    "for row in csv_stances:\n",
    "    body = bodies[int(row['Body ID'])]\n",
    "    if row['Stance'] == 'unrelated':\n",
    "        all_unrelated.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'discuss':\n",
    "        all_discuss.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'agree':\n",
    "        all_agree.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'disagree':\n",
    "        all_disagree.append((row['Headline'], body, row['Stance']))\n",
    "f_stances.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train and validation\n",
    "print('\\tUnrltd\\tDiscuss\\t Agree\\tDisagree')\n",
    "print('All\\t', len(all_unrelated), '\\t', len(all_discuss), '\\t', len(all_agree), '\\t', len(all_disagree))\n",
    "\n",
    "train_unrelated = all_unrelated[:len(all_unrelated) * 9 // 10]\n",
    "train_discuss = all_discuss[:len(all_discuss) * 9 // 10]\n",
    "train_agree = all_agree[:len(all_agree) * 9 // 10]\n",
    "train_disagree = all_disagree[:len(all_disagree) * 9 // 10]\n",
    "\n",
    "val_unrelated = all_unrelated[len(all_unrelated) * 9 // 10:]\n",
    "val_discuss = all_discuss[len(all_discuss) * 9 // 10:]\n",
    "val_agree = all_agree[len(all_agree) * 9 // 10:]\n",
    "val_disagree = all_disagree[len(all_disagree) * 9 // 10:]\n",
    "\n",
    "print('Train\\t', len(train_unrelated), '\\t', len(train_discuss), '\\t', len(train_agree), '\\t', len(train_disagree))\n",
    "print('Valid.\\t', len(val_unrelated), '\\t', len(val_discuss), '\\t', len(val_agree), '\\t', len(val_disagree))\n",
    "\n",
    "train_all = np.array(train_unrelated + train_discuss + train_agree + train_disagree)  # each article = (headline, body, stance)\n",
    "random.Random(0).shuffle(train_all)\n",
    "val_all = np.array(val_unrelated + val_discuss + val_agree + val_disagree)\n",
    "random.Random(0).shuffle(val_all)\n",
    "print('Train (Total)', train_all.shape, '\\tValidation (Total)', val_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLoVe word vectors\n",
    "f_glove = open(\"data/glove.6B.50d.txt\", \"rb\")  # download from https://nlp.stanford.edu/projects/glove/\n",
    "# also try if higher-dimension/higher-vocabulary GLoVe vectors work better\n",
    "glove_vectors = {}\n",
    "for line in tqdm(f_glove):\n",
    "    glove_vectors[str(line.split()[0]).split(\"'\")[1]] = np.array(list(map(float, line.split()[1:])))\n",
    "# for key, value in glove_vectors.items():\n",
    "#    print(key, value)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus of article bodies and headlines in training dataset\n",
    "corpus = np.r_[train_all[:, 1], train_all[:, 0]]  # 0 to 44973 are bodies, 44974 to 89943 are headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert documents in training set to tf-idf form, to learn idf weights\n",
    "tf_idf_vec = TfidfVectorizer(stop_words='english')\n",
    "tf_idf = tf_idf_vec.fit_transform(corpus)\n",
    "# print(tf_idf[0])\n",
    "print(tf_idf.shape)  # (2 x no of docs, no of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index of column indices in tf_idf -> word\n",
    "col_to_word = {i[1]: i[0] for i in tf_idf_vec.vocabulary_.items()}\n",
    "print(list(col_to_word.items())[:5])\n",
    "# vocab = tf_idf_vec.vocabulary_\n",
    "# print(list(vocab.items())[:5])\n",
    "# idf = {word: tf_idf_vec.idf_[vocab[word]] for word in vocab}\n",
    "# print(list(idf.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a document to GloVe vectors, by computing tf-idf of each word * GLoVe of word / total tf-idf for document\n",
    "def doc_to_glove(doc):\n",
    "    doc_tf_idf = tf_idf_vec.transform([doc])\n",
    "    # print(doc_tf_idf[:10])\n",
    "    _, cols = doc_tf_idf[0].nonzero()\n",
    "    doc_vector = np.array([0.0]*50)\n",
    "    sum_tf_idf = 0\n",
    "    for col in cols:\n",
    "        word = col_to_word[col]\n",
    "        if word in glove_vectors:\n",
    "            # print(word, tf_idf[row, col], glove_vectors[word])\n",
    "            doc_vector += glove_vectors[word] * doc_tf_idf[0, col]\n",
    "            sum_tf_idf += doc_tf_idf[0, col]\n",
    "    doc_vector /= sum_tf_idf\n",
    "    return doc_vector\n",
    "print(train_all[0,0])\n",
    "print(doc_to_glove(train_all[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity of GLoVe vectors for all headline-body pairs\n",
    "def cosine_similarity(doc):\n",
    "    return 1.0 - cosine(doc_to_glove(doc[0]), doc_to_glove(doc[1]))\n",
    "\n",
    "for i in range(3):\n",
    "    # unrelated should have lower than rest\n",
    "    print(cosine_similarity(train_unrelated[i]), train_unrelated[i][2])\n",
    "    print(cosine_similarity(train_discuss[i]), train_discuss[i][2])\n",
    "    print(cosine_similarity(train_agree[i]), train_agree[i][2])\n",
    "    print(cosine_similarity(train_disagree[i]), train_disagree[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the KL-Divergence of language model (LM) representations of the headline and the body\n",
    "eps = 0.1  # add a small value for every common word in the LM, as KL-divergence won't work if there are 0 values\n",
    "def kl_divergence(doc):\n",
    "    # Convert body and headline to bag of words representations\n",
    "    vec = CountVectorizer(stop_words='english')\n",
    "    vec_all = vec.fit_transform([doc[0], doc[1]])\n",
    "    vec_headline = np.squeeze(np.array(vec_all[0].todense()))\n",
    "    vec_body = np.squeeze(np.array(vec_all[1].todense()))\n",
    "    \n",
    "    # Compute a simple unigram LM of headline and body using bag of words / no. of words in doc\n",
    "    lm_headline = (vec_headline + eps) / np.sum(vec_headline)\n",
    "    lm_body = (vec_body + eps) / np.sum(vec_body)\n",
    "    \n",
    "    # Return KL-divergence of lm_\n",
    "    return entropy(lm_headline, lm_body)\n",
    "\n",
    "for i in range(3):\n",
    "    # unrelated should have higher than rest\n",
    "    print(kl_divergence(train_unrelated[i]), train_unrelated[i][2])\n",
    "    print(kl_divergence(train_discuss[i]), train_discuss[i][2])\n",
    "    print(kl_divergence(train_agree[i]), train_agree[i][2])\n",
    "    print(kl_divergence(train_disagree[i]), train_disagree[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO other features (use fnc-baseline's n-grams + entities (maybe polarity of sentences containing entities in body also))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to convert (headline, body) to feature vectors for each document\n",
    "ftrs = [cosine_similarity, kl_divergence]\n",
    "def to_feature_array(doc):\n",
    "    vec = np.array([0.0] * len(ftrs))\n",
    "    for i in range(len(ftrs)):\n",
    "        vec[i] = ftrs[i](doc)\n",
    "    return vec\n",
    "\n",
    "# Initialise x and y for train dataset\n",
    "x_train = np.array([to_feature_array(doc) for doc in tqdm(train_all)])\n",
    "print(x_train[:5])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(list(train_all[:, 2]))\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GLoVe distance vs KL-Divergence on a coloured scatter plot with different colours for each label\n",
    "colours = np.array(['b', 'g', 'r', 'y'])\n",
    "plt.scatter(list(x_train[:, 0]), list(x_train[:, 1]), c=colours[y_train])\n",
    "plt.xlabel('Cosine Similarity of GLoVe vectors')\n",
    "plt.ylabel('KL Divergence of Unigram LMs')\n",
    "print([(colours[i], le.classes_[i]) for i in range(len(le.classes_))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement linear/logistic regression classifier using these features. Optimise params on validation set\n",
    "clf = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)  # TODO temporary classifier\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise x and y for validation dataset\n",
    "x_val = np.array([to_feature_array(doc) for doc in tqdm(val_all)])\n",
    "print(x_val[:5])\n",
    "\n",
    "y_val = le.transform(list(val_all[:, 2]))\n",
    "print(y_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict y for validation set\n",
    "y_pred = clf.predict(x_val)\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare y_pred and y for validation set\n",
    "actual = le.inverse_transform(y_val)\n",
    "predicted = le.inverse_transform(y_pred)\n",
    "confusion_matrix(actual, predicted)\n",
    "# TODO save to .csv (see data/scorer.py)\n",
    "# score, cm = score_submission(actual, predicted)\n",
    "# print(score)\n",
    "# print_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO analyse importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused code, could be useful again later\n",
    "\n",
    "# Generate language model representations of headlines and bodies (bag of words / no. of words in doc for a 1-gram LM)\n",
    "# Sparse matrix division, because regular '/' would convert it to dense which will run out of memory\n",
    "# Equivalent to lang_models = bag_of_words/bag_of_words.sum(axis=1)[:, 0] \n",
    "# (Credit to https://stackoverflow.com/questions/42225269/scipy-sparse-matrix-division)\n",
    "b = sparse.bsr_matrix(bag_of_words)\n",
    "c = sparse.diags(1/b.sum(axis=1).A.ravel())\n",
    "lang_models = c @ b\n",
    "\n",
    "print(lang_models.sum(axis=1)) # every element should be 1\n",
    "print(lang_models.shape, bag_of_words.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
