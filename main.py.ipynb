{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from data.scorer import score_submission, print_confusion_matrix, score_defaults, SCORE_REPORT\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import cosine  # TODO may have to implement own maths formulas\n",
    "from scipy.stats import entropy  # (kl-divergence) TODO may have to implement own maths formulas\n",
    "from sklearn.linear_model import LogisticRegression  # TODO implement own classifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # TODO implement own tokenizer + count + tf-idf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder  # TODO implement my own mapping\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "f_bodies = open('data/train_bodies.csv', 'r', encoding='utf-8')\n",
    "csv_bodies = csv.DictReader(f_bodies)\n",
    "bodies = []\n",
    "for row in csv_bodies:\n",
    "    body_id = int(row['Body ID'])\n",
    "    if (body_id + 1) > len(bodies):\n",
    "        bodies += [None] * (body_id + 1 - len(bodies))\n",
    "    bodies[body_id] = row['articleBody']\n",
    "f_bodies.close()\n",
    "body_inverse_index = {bodies[i]: i for i in range(len(bodies))}\n",
    "\n",
    "all_unrelated, all_discuss, all_agree, all_disagree = [], [], [], []  # each article = (headline, body, stance)\n",
    "\n",
    "f_stances = open('data/train_stances.csv', 'r', encoding='utf-8')\n",
    "csv_stances = csv.DictReader(f_stances)\n",
    "for row in csv_stances:\n",
    "    body = bodies[int(row['Body ID'])]\n",
    "    if row['Stance'] == 'unrelated':\n",
    "        all_unrelated.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'discuss':\n",
    "        all_discuss.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'agree':\n",
    "        all_agree.append((row['Headline'], body, row['Stance']))\n",
    "    elif row['Stance'] == 'disagree':\n",
    "        all_disagree.append((row['Headline'], body, row['Stance']))\n",
    "f_stances.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train and validation\n",
    "print('\\tUnrltd\\tDiscuss\\t Agree\\tDisagree')\n",
    "print('All\\t', len(all_unrelated), '\\t', len(all_discuss), '\\t', len(all_agree), '\\t', len(all_disagree))\n",
    "\n",
    "train_unrelated = all_unrelated[:len(all_unrelated) * 9 // 10]\n",
    "train_discuss = all_discuss[:len(all_discuss) * 9 // 10]\n",
    "train_agree = all_agree[:len(all_agree) * 9 // 10]\n",
    "train_disagree = all_disagree[:len(all_disagree) * 9 // 10]\n",
    "\n",
    "val_unrelated = all_unrelated[len(all_unrelated) * 9 // 10:]\n",
    "val_discuss = all_discuss[len(all_discuss) * 9 // 10:]\n",
    "val_agree = all_agree[len(all_agree) * 9 // 10:]\n",
    "val_disagree = all_disagree[len(all_disagree) * 9 // 10:]\n",
    "\n",
    "print('Train\\t', len(train_unrelated), '\\t', len(train_discuss), '\\t', len(train_agree), '\\t', len(train_disagree))\n",
    "print('Valid.\\t', len(val_unrelated), '\\t', len(val_discuss), '\\t', len(val_agree), '\\t', len(val_disagree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = train_unrelated + train_discuss + train_agree + train_disagree  # each article = (headline, body, stance)\n",
    "random.Random(0).shuffle(train_all)\n",
    "train_all = np.array(train_all)\n",
    "\n",
    "val_all = val_unrelated + val_discuss + val_agree + val_disagree\n",
    "random.Random(0).shuffle(val_all)\n",
    "val_all = np.array(val_all)\n",
    "\n",
    "print('Train (Total)', train_all.shape, '\\tValidation (Total)', val_all.shape)\n",
    "print(np.count_nonzero(train_all[:, 2] == 'unrelated'), '\\t',\n",
    "      np.count_nonzero(train_all[:, 2] == 'discuss'), '\\t',\n",
    "      np.count_nonzero(train_all[:, 2] == 'agree'), '\\t',\n",
    "      np.count_nonzero(train_all[:, 2] == 'disagree'))\n",
    "print(np.count_nonzero(val_all[:, 2] == 'unrelated'), '\\t',\n",
    "      np.count_nonzero(val_all[:, 2] == 'discuss'), '\\t',\n",
    "      np.count_nonzero(val_all[:, 2] == 'agree'), '\\t',\n",
    "      np.count_nonzero(val_all[:, 2] == 'disagree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLoVe word vectors\n",
    "f_glove = open(\"data/glove.6B.50d.txt\", \"rb\")  # download from https://nlp.stanford.edu/projects/glove/\n",
    "# also try if higher-dimension/higher-vocabulary GLoVe vectors work better\n",
    "glove_vectors = {}\n",
    "for line in tqdm(f_glove):\n",
    "    glove_vectors[str(line.split()[0]).split(\"'\")[1]] = np.array(list(map(float, line.split()[1:])))\n",
    "# for key, value in glove_vectors.items():\n",
    "#    print(key, value)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors['glove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus of article bodies and headlines in training dataset\n",
    "corpus = np.r_[train_all[:, 1], train_all[:, 0]]  # 0 to 44973 are bodies, 44974 to 89943 are headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert documents in training set to tf-idf form, to learn idf weights\n",
    "tf_idf_vec = TfidfVectorizer(stop_words='english')\n",
    "tf_idf = tf_idf_vec.fit_transform(corpus)\n",
    "# print(tf_idf[0])\n",
    "print(tf_idf.shape)  # (2 x no of docs, no of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index of column indices in tf_idf -> word\n",
    "col_to_word = {i[1]: i[0] for i in tf_idf_vec.vocabulary_.items()}\n",
    "print(list(col_to_word.items())[:5])\n",
    "# idf = {word: tf_idf_vec.idf_[vocab[word]] for word in tf_idf_vec.vocabulary_}\n",
    "# print(list(idf.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a document to GloVe vectors, by computing tf-idf of each word * GLoVe of word / total tf-idf for document\n",
    "def doc_to_glove(doc):\n",
    "    doc_tf_idf = tf_idf_vec.transform([doc])\n",
    "    # print(doc_tf_idf[:10])\n",
    "    _, cols = doc_tf_idf[0].nonzero()\n",
    "    doc_vector = np.array([0.0]*50)\n",
    "    sum_tf_idf = 0\n",
    "    for col in cols:\n",
    "        word = col_to_word[col]\n",
    "        if word in glove_vectors:\n",
    "            # print(word, tf_idf[row, col], glove_vectors[word])\n",
    "            doc_vector += glove_vectors[word] * doc_tf_idf[0, col]\n",
    "            sum_tf_idf += doc_tf_idf[0, col]\n",
    "    doc_vector /= sum_tf_idf\n",
    "    return doc_vector\n",
    "print(train_all[0,0])\n",
    "print(doc_to_glove(train_all[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity of GLoVe vectors for all headline-body pairs\n",
    "def cosine_similarity(doc):\n",
    "    return 1.0 - cosine(doc_to_glove(doc[0]), doc_to_glove(doc[1]))\n",
    "\n",
    "for i in range(10):\n",
    "    # unrelated should have lower than rest\n",
    "    print(cosine_similarity(train_all[i]), train_all[i, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the KL-Divergence of language model (LM) representations of the headline and the body\n",
    "eps = 0.1  # add a small value for every common word in the LM, as KL-divergence won't work if there are 0 values\n",
    "def kl_divergence(doc):\n",
    "    # Convert body and headline to bag of words representations\n",
    "    vec = CountVectorizer(stop_words='english')\n",
    "    vec_all = vec.fit_transform([doc[0], doc[1]])\n",
    "    vec_headline = np.squeeze(np.array(vec_all[0].todense()))\n",
    "    vec_body = np.squeeze(np.array(vec_all[1].todense()))\n",
    "    \n",
    "    # Compute a simple unigram LM of headline and body using bag of words / no. of words in doc\n",
    "    lm_headline = (vec_headline + eps) / np.sum(vec_headline)\n",
    "    lm_body = (vec_body + eps) / np.sum(vec_body)\n",
    "    \n",
    "    # Return KL-divergence of lm\n",
    "    return entropy(lm_headline, lm_body)\n",
    "\n",
    "for i in range(20):\n",
    "    # unrelated should have lower than rest\n",
    "    print(cosine_similarity(train_all[i]), train_all[i, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO other features (use shared n-grams, shared entities, polarity of sentences containing entities in body)\n",
    "# Features should return a float value between 0 and 1\n",
    "\n",
    "def unigram_match(doc):\n",
    "    # Returns how many times 1-grams that occur in the article's headline occur on the article's body.\n",
    "    vec = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "    vec.fit([doc[0]])\n",
    "    vec_body = vec.transform([doc[1]])\n",
    "    # Make the range 0-1 and log-scaled (difference between 0 and 10 matches > 100 and 110)\n",
    "    return np.power((np.sum(vec_body) / len(doc[1])), 1 / np.e)\n",
    "    \n",
    "for i in range(10):\n",
    "    # unrelated should have lower than rest\n",
    "    print(unigram_match(train_all[i]), train_all[i, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_match(doc):\n",
    "    # get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to convert (headline, body) to feature vectors for each document\n",
    "ftrs = [cosine_similarity, kl_divergence, unigram_match]\n",
    "def to_feature_array(doc):\n",
    "    vec = np.array([0.0] * len(ftrs))\n",
    "    for i in range(len(ftrs)):\n",
    "        vec[i] = ftrs[i](doc)\n",
    "    return vec\n",
    "\n",
    "# Initialise x and y for train dataset\n",
    "x_train = np.array([to_feature_array(doc) for doc in tqdm(train_all)])\n",
    "print(x_train[:5])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(list(train_all[:, 2]))\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GLoVe distance vs KL-Divergence on a coloured scatter plot with different colours for each label\n",
    "colours = np.array(['g', 'r', 'b', 'y'])\n",
    "plt.scatter(list(x_train[:, 0]), list(x_train[:, 1]), c=colours[y_train])\n",
    "plt.xlabel('Cosine Similarity of GLoVe vectors')\n",
    "plt.ylabel('KL Divergence of Unigram LMs')\n",
    "print([(colours[i], le.classes_[i]) for i in range(len(le.classes_))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise x and y for validation dataset\n",
    "x_val = np.array([to_feature_array(doc) for doc in tqdm(val_all)])\n",
    "print(x_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement linear/logistic regression classifier using these features. Optimise params on validation set\n",
    "clf = LogisticRegression(C=1e2, random_state=0, multi_class='multinomial', solver='saga')\n",
    "# clf = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)  # TODO temporary classifier\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict y for validation set\n",
    "y_pred = clf.predict(x_val)\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset format for score_submission in scorer.py\n",
    "predicted = le.inverse_transform(y_pred)\n",
    "body_ids = [str(body_inverse_index[body]) for body in val_all[:, 1]]\n",
    "pred_for_cm = np.array([{'Headline': val_all[i, 0], 'Body ID': body_ids[i], 'Stance': predicted[i]} for i in range(len(val_all))])\n",
    "gold_for_cm = np.array([{'Headline': val_all[i, 0], 'Body ID': body_ids[i], 'Stance': val_all[i, 2]} for i in range(len(val_all))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score using scorer.py:\n",
    "test_score, cm = score_submission(gold_for_cm, pred_for_cm)\n",
    "null_score, max_score = score_defaults(gold_for_cm)\n",
    "print_confusion_matrix(cm)\n",
    "print(SCORE_REPORT.format(max_score, null_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to .csv in the correct format (see data/scorer.py and data/train_stances.csv)\n",
    "pred_tosave = np.array([('Headline','Body ID','Stance')] + [(val_all[i, 0], body_ids[i], predicted[i]) for i in range(len(val_all))])\n",
    "gold_tosave = np.array([('Headline','Body ID','Stance')] + [(val_all[i, 0], body_ids[i], val_all[i, 2]) for i in range(len(val_all))])\n",
    "\n",
    "# TODO does not work ATM (UnicodeDecodeError) - try 'python scorer.py val_gold.csv val_predicted.csv' in data folder\n",
    "np.savetxt('data/val_predicted.csv', pred_tosave, delimiter=',', fmt='%s', encoding='utf-8', newline='\\n')\n",
    "np.savetxt('data/val_gold.csv', gold_tosave, delimiter=',', fmt='%s', encoding='utf-8', newline='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO analyse importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoostingClassifier(n_estimators=200)\n",
    "ACCURACY: 0.834  ||  1604.25  ||  No Added Features (just Cosine and KL-Divergence)\n",
    "ACCURACY: 0.861  ||  1743.5   ||  'Raw' Unigram Match\n",
    "ACCURACY: 0.861  ||  1743.5   ||  1+log(x) Unigram Match\n",
    "ACCURACY: 0.860  ||  1743.5   ||  1-1/e^(x/2) Unigram Match\n",
    "ACCURACY: 0.884  ||  1834.5   ||  x/len(body) Unigram Match\n",
    "ACCURACY: 0.873  ||  1790.5   ||  log(x)/log(len(body)) Unigram Match\n",
    "ACCURACY: 0.884  ||  1834.25  ||  (x/len(body))^1/e Unigram Match\n",
    "    \n",
    "LogisticRegression(C=1e5)\n",
    "ACCURACY: 0.880  ||  1795.5   ||  (x/len(body))^1/e Unigram Match\n",
    "\n",
    "LogisticRegression(C=1e2, multi_class='multinomial', solver='saga')\n",
    "ACCURACY: 0.881  ||  1818.5   ||  (x/len(body))^1/e Unigram Match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
